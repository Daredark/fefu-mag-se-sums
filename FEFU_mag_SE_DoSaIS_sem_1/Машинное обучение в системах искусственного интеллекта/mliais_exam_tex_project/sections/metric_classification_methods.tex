\section{Метрические методы классификации}

\subsection{2. Содержание}

1. Определение расстояний между объектами
    - Метрика, метрическое пространство
    - Гипотезы компактности и непрерывности.
2. Метрические методы классификации
    - Обобщенный метрический классификатор
    - Метод ближайших соседей и его обобщения
    - Метод окна Парзена, метод понетциальных функций
3. Отбор эталонов и признаков, оптимизация метрики
    - Понятие отступа, алгоритм отбора эталонных объектов STOLP
    - Задача выбора метрики, жадный алгоритм отбора признаков
    - Полный скользящий контроль CCV (другая презентация)

\subsection{3. Гипотеза компактности}

\textbf{Гипотеза компактности} - в задачах классификации предположение о том,
что схожие объекты гораздо чаще лежат в одном классе, чем в разных.
Классы образуют компактно локализованные подмножества в пространстве
объектов.
Граница между классами имеет достаточно простую форму.

В математическом анализе \textit{компактными} называются ограниченные замкнутные
множества.
\textit{Гипотеза компактности} не имеет ничего общего с этим понятием и должна
пониматься в "более бытовом" смысла этого слова.

Для формализации понятия "сходства" вводится функция расстояния или
\textbf{метрика} $\rho (x, x')$ в пространстве объектов $X$.
Алгоритмы, основанные на анализе сходства объектов, называют
\textbf{метрическими}, даже тогда, когда функция $\rho$ не удовлетворяет всем
аксиомам метрики (чаще - неравенству треугольника).

\subsection{4. Метрическое пространство}

\textbf{Метрикой} на множестве $X$ называется отображение $d \!: X \times X
\rightarrow R$
сопоставляющее каждой паре $(x, y) \in X \times X$ вещественное число
$d(x,y)$, удовлетворяющее условиям:
- неотрицательность: $d(x, y)\geq 0 $ для любых $(x,y)$;
- $d(x,y)=0$ тогда и только тогда, когда $x=y$;
- симметричность: $d(x,y)=d(y,x)$;
- неравенство треугольника:
$$d(x,y) \leq d(x,z) + d(z,y) \;\; \text{для любых} \;\; x, y, z \in X.$$

Множество $X$ вместе с отображением $d$ называется
\textbf{метрическим пространством}, и обозначается $(X, d)$.

Метрика является обобщением понятия \textit{расстояния} на произвольные
пространства.
Всякое пространство может быть наделено метрикой.

Методы основаны на предположении, что \textbf{эксперт уже построил достаточно
адекватную метрику}, для которой выполняется гипотеза компактности.
Выбор адекватности метрики является наиболее сложной и наименее
исследованной подзадачей (возможен автоматический подбор).

\subsection{5. Гипотезы для объектов}

\textbf{Задачи классификации и регрессии:}
- $X$ - объекты, $Y$ - ответы;
- $X^l = {(x_i, y_i)}^l_{i=1}$ - обучающая выборка;

\textbf{Гипотеза непрерывности} (для регрессии):
\
\textit{близким объектам соответствуют близкие ответы.}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Гипотеза компактности} (для классификации):
\
\textit{близкие объекты, как правило, лежат в одном классе.}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Регрессия}: каждый следующий объект связан с предыдущим некоторой
(непрерывной) функцией.

\textbf{Классификация}: объекты образуют компактные сгустки.

\subsection{6. Пример: ирисы Фишера}

Привычная мера близости - евклидова метрика в $\mathbb{R}^2$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Выборка}: 150 экземпляров ириса, по 50 экземплярова из трех видов:
- Ирис щетинистый;
- Ирис разноцветный;
- Ирис виргинский.

Для каждого экземпляра измерялись 4 характеристики (в сантиметрах).

\subsection{7. Формализация понятия "близости"}

Евклидова метрика и обобщенная метрика Минковского:

$$\rho(x,x_i) = {\left( \sum_{j=1}^n{\left| x^j - x^j_i
\right|}^2\right)}^{1/2} \qquad \rho(x,x_i) = {\left(
\sum_{j=1}^n{w_j \left| x^j - x^j_i \right|}^p\right)}^{1/p}$$

- $x = (x^1, \ldots, x^n)$ - вектор признаков объекта $x$,
- $x_i = (x_i^1, \ldots, x_i^n)$ - вектор признаков объекта $x_i$,

$w_1, \ldots, w_n$ - веса признаков, которое можно обучать.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Вводится функция расстояния (метрика).}

Делать это можно по-разному:
1. Через масштабный коэффициент - веса признаков (км-кг, 1К-0,1 и т.д.).
2. Через показатель $p$ (рисунки слайда).

\subsection{8. Обобщенный метрический классификатор}

Для произвольного $x \in X$ отранжируем объекты $x_1, \ldots, x_l$:
$$\rho(x, x^{(1)}) \leqslant \rho(x, x^{(2)}) \leqslant \ldots \leqslant
\rho(x, x^{(l)}),$$

- $x^{(i)}$ - $i$-й сосед объекта $x$ среди $x_1, \ldots, x_l$;
- $y^{(i)}$ - ответ на $i$-м соседе объекта $x$.

\textbf{Метрический алгоритм классификации:}

$$\displaystyle \text{a}(x;X^l) = {\underset{x}{\arg\max}}_{y \in Y} \underbrace{\sum^l_{i=1}
{[y^{(i)} = y] \; w(i, x)}}_{Г_y(x)},$$

- $w(i,x)$ - вес (степень важности) $i$-го соседа объекта $x$,
неотрицателен, не возрастает по $i$.
- $Г_y(x)$ - \textit{оценка близости} объекта $x$ к классу $y$.

\textbf{Принцип}:
для классифицируемого объекта смотреть, рядом с какими объектами он
находится, и к каким они относятся классам.

${\underset{x}{\arg\max}}$ - значение аргумента, при котором данное выражение достигает
максимума.

\subsection{9. Метод k ближайших соседей}

- $w(i,x) = [i \leqslant 1]$ - метод ближайшего соседа
- $w(i,x) = [i \leqslant k]$ - метод $k$ ближайших соседей

\textbf{Преимущества:}

- простота реализации (lazy learning);
- параметр $k$ можно оптимизировать по критерию скользящего контроля
(leave-one-out):
$$\text{LOO}{(k, X^l)} = \sum^l_{i=1}{\left[ \text{a}
{\left(x_i; X^l \backslash \lbrace x_i \rbrace, k \right)}
\neq y_i \right]} \rightarrow \min_k.$$

\textbf{Недостатки:}
- неоднозначность классификации при $Г_y(x) = Г_s(x), \;\; y \neq s$.
- не учитываются значения расстояний

\textbf{k nearest neighbors (kNN)}

При $k=1$ как такового обучения нет - приходится запоминать всю выборку.

Интерпретируемость (медицина, геология).

Неустойчивость к шуму и выбросам.

\subsection{10. Зависимости LOO от числа соседей}

\textbf{Пример.} Задача UCI: Iris.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

- смещенное число ошибок, когда объект
учитывается как сосед самого себя
- несмещенное число ошибок LOO

\textbf{Leave-one-out (LOO).}

Нужно выбирать нижние точки графика, при этом чтобы значение $k$ было не
очень велико.

\subsection{11. Метод k взвешенных ближайших соседей}

$$w(i,x) = [i \leqslant k]w_i,$$
где $w_i$ - вес, зависящий только от номера соседа;

\textbf{Возможные эвристики:}
- $w_i = \frac{k+1-i}{k}$ - линейные убывающие веса;
- $w_i = q^i$ - экспоненциально убывающие веса, $0 < q < 1$;

\textbf{Проблемы:}
- как более обоснованно задать веса?
- возможно, было бы лучше, если бы вес $w(i,x)$ зависел не от порядкового
номера соседа $i$, а от расстояния до него $\rho (x, x^{(i)})$.

Попытка устранить неоднозначность, когда соседей пополам в одном и в другом
классах.

Если номера 1 и 4 в одном классе, а 2 и 3 - в другом, то основа
неоднозначность.

Нужно учитывать расстояние, оно может меняться в разы, а то и на порядки.

\subsection{12. Метод окна Парзена}

- $\displaystyle w(i,x) = K{\left(\frac{\rho(x, x^{(i)})}{h}\right)}$, где
$h$ - ширина окна,
- $K(r)$ - ядро, не возрастает и положительно на $[0,1]$.

Метод парзеновского окна \textit{фиксированной ширины}:

$$\text{a}(x; X^l, h, K) = {\underset{x}{\arg\max}}_{y \in Y}{\sum^l_{i=1}{[y_i = y] K
\left( \frac{\rho(x, x_i)}{h} \right)}}$$

Метод парзеновского окна \textit{переменной ширины}:

$$\text{a}(x; X^l, k, K) = {\underset{x}{\arg\max}}_{y \in Y}{\sum^l_{i=1}{[y_i = y] K
\left( \frac{\rho(x, x_i)}{\rho{(x, x^{(k+1)})}} \right)}}$$

\textbf{Оптимизация параметров} - по критерию LOO:
- выбор ширины окна $h$ или числа соседей $k$
- выбор ядра $K$

$h$ - радиус, дальше которого объекты не рассматриваются.

$k$ - элементы, которые должны "влезть в окно"

Переменная ширина окна применяется при неравномерности выборки
(часть - сгустками, часть - разрежена).

\subsection{13-18. Парзеновское окно фиксированной ширины h}

\textbf{Пример:} двумерная выборка, два класса $Y = \lbrace -1, +1 \rbrace$.

$$\text{a}(x) = {\underset{x}{\arg\max}}_{y \in Y} Г_y(x) = \text{sign}
\underbrace{\left( Г_{+1}(x) - Г_{-1}(x) \right)}$$

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{19. Метод потенциальных функций}

$$w(i, x) = \gamma^{(i)} K{\left( \frac{\rho(x, x^{(i)})}{h^{(i)}}
\right)}$$

Более простая запись (здесь можно не ранжировать объекты):

$$\text{a}(x;X^l) = {\underset{x}{\arg\max}}_{y \in Y} \sum_{i=1}^l{\left[ y_i = y \right]
\gamma_i K \left( \frac{\rho(x, x_i)}{h_i} \right)},$$

где $\gamma_i$ - веса объектов, $\gamma_i \geqslant 0$, $h_i > 0$.

\textbf{Физическая аналогия} из электростатики:
- $\gamma_i$ - величина "заряда" в точке $x_i$;
- $h_i$ - "радиус действия" потенциала с центром в точке $x_i$;
- $y_i$ - знак "заряда" (в случае двух классов $Y = \{ -1, +1 \}$);
- $K(r) = \frac{1}{r}$ или $\frac{1}{r+\text{a}}$

В задачах классификации нет ограничений ни на $K$, ни на $|Y|$.

В каждый объект обучающей выборки поставим шапочку:
- в классе +1 $\wedge$,
- в классе -1 $\vee$.

Возникает физическая аналогия этой задачи с электостатикой:
есть точки (точечные заряды) - одни положительные, другие отрицательные.
Возникает потенциал (там, где он нулевой - разделяющая поверхность).

\subsection{Общая на всю лекцию картинка}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{Типы объектов в зависимости от отступа}

Типы объектов в зависимости от отступа:
- \textbf{Эталон} - это элемент, находящийся "в глубине" своего класса, либо
максимально далеко от элементов других классов
- \textbf{Пограничные объекты}
- \textbf{Неинформативные элементы} (рядом с эталонными)
- \textbf{Выброс} - это ???
- \textbf{Шум} - это ???

Это были все группы, на которые можно разбить элементы каждого класса, в
зависимости от чего-то.

Эталонные объекты - это образец: что-то, с чем мы сравниваем.

И выброс, и шум являются \textbf{ошибкой}.

\subsection{Отступ (margin) M}

\textbf{Отступ (margin)} ($M$) - расстояние элемента от границы класса.

Если объект имеет положительный отступ и этот $M$ очень большой, тогда мы
назовем его эталонным.

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        Группы объектов & Значение группы & Обозначение \\ \hline
        эталон & очень большое значение & \$M+\$ \\ \hline
        пограничные & малое значение & \$M+\$ \\ \hline
        неинформативные & среднее значение & \$M+\$ \\ \hline
        выброс & большое значение & \$M-\$ \\ \hline
        шум & малое значение & \$M-\$ \\ \hline
    \end{tabular}
\end{table}

\subsection{Алгоритм СТОЛП (STOLP)}

\textbf{Алгоритм СТОЛП (STOLP)} - это алгоритм отбора эталонных объектов для
\textit{метрического классификатора}.

Этапы алгоритма СТОЛП:
1. Выбрать эталоны
2. Убрать все выбросы, на которых большой $M-$
3. Выстроить границу между классами (пограничные элементы)
4. Все отсальные элементы можно удалить - их можно считать неинформативными
(ну и шумы тоже)

\subsection{Дополнительное пояснение к группам объектов по отступам}

\textbf{Исключение} - это выбивающееся из закономерности, факт изъяна нашего
правила.

\textbf{Отступ} - это велечина, которая показывает степень типичности объекта.

- \textbf{Выброс} - это то, чего не может быть: большое значение отступа и
находится в другом классе
- Самый типичный - \textbf{эталон}
- \textbf{Неинформативность} - заурядность
- \textbf{Шум} - это пограничный объект, но по ту сторону

\subsection{Преимущества и недостатки алгоритма СТОЛП (STOLP)}

\textbf{Преимущества алгоритма}:
- сокращается число хранимых объектов;
- сокращается время классификации.

\textbf{Недостатки алгоритма}:
- нужно задать вручную параметр $\delta$ (дельта).

\subsection{Прочая информация}

Мы используем Алгоритм СТОЛП (STOLP), чтобы избавиться от того количества
элементов, которое могло бы усложнить (увеличить время) выполнения чего-то.

Можно избавиться от зависимых признаков.

Можно попробовать одним признаком классифицировать вообще всё (пример:
длина волос при классах мужщина/женщина).

Нужно оставить совокупность самых лучших признаков.

Алгоритм поиска информативных признаков.

\subsection{Разная информация о домашней работе №1}

Общая на всю пару картинка:

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

То, что мы делали до этого, было в точке.
Пора перейти на циклы для настоящего эксперимента.

Чем более подробно описано задание, тем лучше.

Не нужно использовать неуместные жаргонизмы.
Вообще лучше не использовать жаргоны.
- Выборка данных вместо Датасет (например).

Чем проще мы выражаем вещи, тем лучше.

Не шутить шутки в лаборатоных работах.

Если признаков слишком много, нужно заняться отбором лучших.

Умение формализовывать критерии - важно.

Есть ещё \textbf{отложенная (верефицирующая) выборка} (помимо тестовой и
контрольной).

Выборку лучше разбирать поровну по классам.
Пример:
- есть:
    - хорошой 60\% картошки
    - средней 30\% картошки
    - гнилой 10\% картошки
- train/test - 80\%/20\%
- тогда
    - train хорошая - 0.8 * 0.6
    - train средняя - 0.8 * 0.3
    - train гнилая - 0.8 * 0.1
    - test хорошая - 0.2 * 0.6
    - test средняя - 0.2 * 0.3
    - test гнилая - 0.2 * 0.1

Сущеуствует такое понятие, как \textbf{полный перебор}.
Из всех возможных диапазонов выбрать только самый лучший (например, 92\%/8\%)

- Можно случайно менять random\_state.
- Можно случайно менять Train/Test.

```
цикл по пропорциям (80-20 и т.д.)
    цикл по сочетаниям (варианты, random\_state)
        ЭКСПЕРИМЕНТ
```

Домашнее задание - организовать полный цикл.
Сделать табличку для сочетаний.