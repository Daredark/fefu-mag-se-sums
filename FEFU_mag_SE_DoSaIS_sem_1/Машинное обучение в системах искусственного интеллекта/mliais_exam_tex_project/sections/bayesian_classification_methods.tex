\section{Байесовские методы классификации}

\subsection{2. Содержание}

Оптимальный байесовский классификатор:
- вероятностная постановка задачи классификации;
- задача восстановления плотности распределения;
- три подхода к оцениванию плотностей;
- наивный байесовский классификатор.

\subsection{3. Вероятностная постановка задачи классификации}

- $X$ - объекты,
- $Y$ - ответы,
- $X \times Y$ - в.п. с плотностью $p{(x, y)}$.

\textbf{Дано}: $X^l = {(x_i, y_i)}_{i=1}^{l} \sim p{(x, y)}$ - простая
выборка.

\textbf{Найти}: $a: \; X \rightarrow Y$ с минимальной вероятностью ошибки.

\textbf{Временное допущение:} пусть известна совместная плотность
$$p{(x, y)} = p{(x)} P{(y \vert x)} = P{(y)} p{(x \vert y)}.$$

- $P{(y)}$ - \textit{априорная вероятность} класса $y$;
- $p{(x \vert y)}$ - \textit{функция правдоподобия} класса $y$;
- $P{(y \vert x)}$ - \textit{апостериорная вероятность} класса $y$.

Априорная вероянтность - вероятность до опыта.

\textbf{Принцип максимума апостериорной вероятности:}
$$a{(x)} = {\underset{x}{\arg\max}}_{y \in Y}{P{(y \vert x)}} = {\underset{x}{\arg\max}}_{y \in Y}{P{(y)} p{(x \vert y)}}.$$

- $p$ - плотность (функция).
- $P$ - вероятность (число).
- $p{(x)}$ - плотность распределения на всем пространстве объектов
(все классы вместе).
- $P{(y)}$ - доля объектов класса $y$ в выборке.
- $p{(x \vert y)}$ - плонтность распределения класса $y$.
- $P{(y \vert x)}$ - вероятность класса $y$ после получения некоторой
информации об объекте $x$ (этого достаточно для его классификации).

\subsection{4. Классификация по максимуму функции правдоподобия}

Частный случай: $\displaystyle a{(x)} = {\underset{x}{\arg\max}}_{y \in Y}{p{(x \vert y)}}$
при равных $P{(y)}$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

$P{(y)}$ равны, т.е. классы равновероятны (иначе разделяющая линия начнет
смещаться в сторону одного из классов).

$p{(x)}$ - это модель классов.

Будем исходить из предположения, что мы умеем моделировать форму классов
(т.е. плотность их распределения).

\subsection{Картинка с доски на всю пару и некоторые термины}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

- Байесовский метод;
- Функция интенсивности (частоты);
- Коеффициент значимости признака.

\subsection{5. Оптимальный байесовский классификатор}

> \textbf{Теорема}\
Пусть $P{(y)}$ и $p{(x \vert y)}$ известны,
$\lambda_y \geqslant 0$ - \textit{потеря от ошибки} на объекте класса $y \in Y$.
Тогда минимум среднего
риска $$\displaystyle R{(a)} = \sum_{y \in Y}{\lambda_{y} \int{{[ a{(x)} \neq y ]} p{(x, y)} dx}}$$
достигается \textit{байесовским классификатором} $$a{(x)} = \max_{y \in Y}{\lambda_y P{(y)} p{(x \vert y)}}.$$

$\lambda_y$ - это штраф за ошибку.

В случае, когда штраф за ошибку попадания объекта в один класс отличается от
штрафа за ошибку попадания в другой класс, $\lambda_y$ представляется
матрицей.

\subsection{6. Задача восстановления плотности распределения}

1. Восстановление плотности распределения по выборке\
\textbf{Дано:} $X^l = {(x_i, y_i)}^{l}_{i=1}$ - обучающая выборка.\
\textbf{Найти:} эмпирические оценки $\hat{P}{(y)}$ и $\hat{p}{(x \vert y)}$,
$y \in Y$.
2. Построение классификатора\
\textbf{Дано:} вероятности $P{(y)}$ и плотности $p{(x \vert y)}$, $y \in Y$.\
\textbf{Найти:} классификатор $a: \; X \times Y$, минимизирующий $R{(a)}$.

\textbf{Замечание 1:} после замены $P{(y)}$ и $p{(x \vert y)}$ их эмпирическими
оценками байесовкий классификатор уже не оптимален.

\textbf{Замечание 2:} задача оценивания плотности распределения - более сложная,
чем задача классификации.

Задача распадается на две подзадачи, и вторая уже решена.

На практике плотности распределения классов, как правило, не известны и их
приходится оценивать (восстанавливать) по обучающей выборке (но с некоторой
погрешностью).

$R{(a)}$ - средний риск.

\subsection{7. Задачи эмпирической оценки P(y) и p(x|y)}

- \textbf{Оценивание априорных вероятностей} частотами
$$\hat{P}{(y)} = \frac{l_y}{l}, \quad l_y = {\vert X_y \vert}, \quad X_y = {\lbrace x_i \in X : \; y_i = y \rbrace}, \quad y \in Y.$$
- \textbf{Оценивание функций правдоподобия:}\
\textbf{Дано:} $X^{m} = {\lbrace x_1, \ldots, x_m \rbrace}$ - простая выборка $(X_y$ без ответов $y_i)$.\
\textbf{Найти:} \textit{эмпирическую оценку плотности} $\hat{p}{(x)}$, аппроксимирующую
истинную плотность $p{(x)}$ на всём $X$:
$$\hat{p}{(x)} \rightarrow p{(x)} \;\; \text{при} \;\; m \rightarrow \infty.$$

%$$
%\begin{split}
%\textit{Это основная задача, для которой далее будут предложены}\\
%\textit{различные варианты ее решения.}
%\end{split}
%$$

Для каждого класса рассматриваются объекты только этого класса, поэтому в
$p{(x \vert y)}$ нет $y$.

Если выборка генерируется независимо, то доля объектов с классом $y$ - это
есть несмещенная оценка вероятности.

$m$ - количество объектов одного из классов.

\subsection{8. Три подхода к оценке плотностей}

1. \textbf{Параметрическое оценивание плотности:}
$$\hat{p}{(x)} = \varphi{(x, \theta)}.$$
2. \textbf{Восстановление смеси распределений:}
$$\hat{p}{(x)} = \sum_{j=1}^{k}{w_j \varphi{(x, \theta_j)}}, \quad k \ll m.$$
3. \textbf{Непараметрическое оценивание плотности:}
$$\hat{p}{(x)} = \sum_{i=1}^{m}{w_i K{\left( \frac{\rho{(x, x_i)}}{h} \right)}}.$$

$\varphi$ - математическая модель плотности (ее нужно угадать).
Параметр $\theta$ нужно подобрать по выборке, он не известен.

Плотность представлена суммой одномерных плотностей ($k$ - число
компонентов смеси), каждая их них с заданной функцией $\varphi$ (со своим
вектором параметров $\theta$).

\subsection{9. Наивный байесовский классификатор}

\textbf{Допущение (действительно наивное):}
Признаки $f_j : \; X \rightarrow D_j$ - независимые случайные величины с
плотностями распределения,
$p_j{(\xi \vert y)}, \; y \in Y, \; j = 1, \ldots, n$.

Тогда функции правдоподобия классов представимы в виде произведения
одномерных плотностей по признакам:
$$p{(x \vert y)} = p_1{(\xi_1 | y)} \cdots p_n{(\xi_n \vert y)}, \quad x = {(\xi_1, \ldots, \xi_n)}, \quad y \in Y.$$

Прологарифмируем (для удобства).
Получим классификатор
$$a{(x)} = \max_{y \in Y}{\left( \ln{\lambda_y \hat{P}{(y)}} + \sum_{j=1}^{n}{\ln{\hat{p}_j{(\xi_j \vert y)}}} \right)}.$$

Восстановление $n$ одномерных плотностей - намного более простая задача, чем
одной $n$-мерной.

Для любой техники восстановления сделаем наивное предположение о том, что
признаки независимы, и что плотность распределения каждого признака в
каждом классе - своя.

$\xi$ - это значения признака на объекте $x$.

\subsection{15. Пример зависимости оценки плотности от ширины окна}

Оценка $\hat{p}_h{(x)}$ при различных значениях ширины окна $h$:

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Вывод:} Качества восстановления плотности существенно зависит от ширины
окна $h$, но слабо зависит от вида ядра $K$.

При уменьшении ширины окна реакция функции плотности осуществляется на
каждый объект (и на его отсутствие - см. распределение объектов по оси $X$),
что приводит к сильным колебаниям.

При увеличении ширины окна результат оказывается "пересглаженным".

\subsection{16. Выбор ширины окна}

Скользящий контроль \textit{Leave One Out}:
$$\text{LOO}{(h)} = \sum_{i=1}^{l}{\left[ a{(x_i; X^l \backslash x_i, h)} \neq y_i \right]} \rightarrow \min_{h},$$

Типичный вид зависимости $\text{LOO}{(h)}$:

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Можно восстановить плотность для разных значений ширины окна, подставить
каждое их них в байесовский классификатор и посмотреть, насколько он
ошибается на каждом из этих значений.

\subsection{17. Окна переменной ширины}

\textbf{Проблема:} при наличии локальных сгущений любая $h$ не оптимальна.

\textbf{Идея:} задавать не ширину окна $h$, а число соседей $k$.
$$h_k{(x)} = \rho{(x, x^{(k+1)})},$$
где $x^{(i)}$ - $i$-й сосед объекта $x$ при ранжировании выборки $X^l$:
$$\rho{(x, x^{(1)}) \leqslant \cdots \leqslant \rho{(x, x^{(l)})}.}$$

\textbf{Замечание 1:} нормировка $V{(h_k)}$ не должна зависеть от $y$, поэтому
выборка ранжируется целиком, а не по классам $X_y$.

\textbf{Замечание 2:} оптимизация $\text{LOO}{(k)}$ аналогична $\text{LOO}{(h)}$.

Если объекты в каких-то местах расположены густо, а в других - нет.

\subsection{18. Краткий конспект (1/2)}

\textbf{Байесовский классификатор} - широкий класс алгоритмов классификации,
основанный на \textit{принципе максимума апостериорной вероятности}.

Данный подход лежит в основе многих удачных алгоритмов классификации:
- наивный байесовский классификатор,
- линейный дискриминант Фишера,
- квадратичный дискриминант,
- метод парзеновского окна,
- логиситческая регрессия,
- метод радиальных базисных функций.

Байесовский подход к классификации является одним из старейших, но до сих
пор успешно применяется при решении некоторых типов задач.

\subsection{19. Краткий конспект (2/2)}

\textbf{Апостериорная вероятность} - условная вероятность случайного события при
условии того, что известны апостериорные данные (т.е. данные, полученные
после опыта).

Для классифицируемого объекта вычисляются функции правдоподобия (плотности
распределения) каждого из классов, и по ним вычисляются апостериорные
вероятности классов.

\textbf{Объект относится к тому классу, для которого апостериорная вероятность
максимальна.}

\textbf{Плотность распределения} показывает то, как часто появляется случайная
величина $X$ в окрестности точки $x$ при повторении опытов.

Зная плотность распределения, можно вычислить вероятность того, что
случайная величина $X$ примет значение, принадлежащее заданному интервалу.