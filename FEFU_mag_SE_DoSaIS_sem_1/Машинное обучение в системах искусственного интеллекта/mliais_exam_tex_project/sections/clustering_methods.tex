\section{1. Лекция 9. Методы кластеризации и частичного обучения}

\subsection{2. Содержание}

Задачи кластеризации и частичного обучения:
- постановка задач, некорректность первой;
- критерии качества кластеризации.

Алгоритмы кластеризации:
- метод $K$-средних ($\textit{K} \text{-means}$);
- алгоритм DBSCAN;
- иерархические методы.

\subsection{3. Постановка задачи кластеризации}

\textbf{Дано:}
- $X$ - пространство объектов;
- $X^l = {\lbrace x_1, \ldots, x_l \rbrace}$ - обучающая выборка;
- $\rho \! : X \times X \rightarrow {\left[ 0, \infty \right)}$ - функция расстояния между объектами.

\textbf{Найти:}
- $Y$ - множество кластеров,
- $a \! : X \rightarrow Y$ - алгоритм кластеризации,
- такие, что:
    - каждый кластер состоит из близких объектов;
    - объекты разных кластеров существенно различны.

Это задача \textit{обучения без учителя} (unsupervised learning).

Дано конечное множество объектов обучающей выборки, для которых указаны
их парные расстояния (без меток классов).

Нужно научиться объединять такие объекты в кластеры (сгустки) по принципу
взаимной близости.

Внутрикластерное расстояние - малое, межкластерное - большое.

\subsection{4. Некорректность задачи кластеризации}

Решение задачи кластеризации принципиально неоднозначно:
- точной постановки задачи кластеризации нет;
- существует много критериев качества кластеризации;
- существует много эвристических методов кластеризации;
- число кластеров $\vert Y \vert$, как правило, неизвестно заранее;
- результат кластеризации сильно зависит от метрики $\rho$, выбор которой
также является эвристикой.

\textbf{Пример:} сколько здесь кластеров?

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Иногда число кластеров задают директивно (из каких-то своих соображений
или условий задачи), при этом далеко не всегда удается понять, сколько
их на самом деле.

На слайде все ответы правильные, потому что результат зависит от того,
насколько далекими друг от друга мы считаем кластеры.

\subsection{5. Цели кластеризации}

- \textbf{Упростить дальнейшую обработку данных}, разбить множество $X^l$ на
группы схожих объектов чтобы работать с каждой группой в отдельности
(задачи классификации, регрессии, прогнозирования).
- \textbf{Сократить объём хранимых данных}, оставив по одному представителю от
каждого кластера (задачи сжатия данных).
- \textbf{Выделить нетипичные объекты}, которые не подходят ни к одному из
кластеров (задачи одноклассовой классификации).
- \textbf{Построить иерархию множества объектов}, пример - классификация
животных и растений К.Линнея (задачи таксономии).

Несмотря на неоднозначность, задачи кластеризации часто используются на
практике.

Карл Линней - создатель единой системы классификации растительного и
животного миров, в которой были обобщены и упорядочены знания всего
предыдущего периода развития биологии.

\subsection{6. Типы кластерных структур}

- внутрикластерные расстояния, как правило, меньше межкластерных
- ленточные кластеры
- кластеры с центром

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Ленточные кластеры нарушают принцип о минимуме внутрикластерного
расстояния и максимуме межкластерного.

С ними бывает наоборот - межкластерное расстояние меньше, чем
внутрикластерное.

\subsection{7. Типы кластерных структур}

- кластеры могут соединяться перемычками
- класетры могут накладываться на разреженный фон из редко расположенных
объектов
- кластеры могут перекрываться

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Наличие перемычек между кластерами приводит к тому, что графовые методы
кластеризации начинают работать допольно плохо (в местах перемычек
кластеры будут разделены).

Шумовые объекты также являются препятствием для алгоритмов кластеризации.

\subsection{8. Типы кластерных структур}

- кластеры могут образовываться не по сходству, а по иным типам
регулярностей
- кластеры могут вообще отсутствовать

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

- Каждый метод кластеризации имеет свои ограничения и выделяет кластеры
лишь некоторых типов.
- Понятие "тип кластерной структуры" зависит от метода и также не имеет
формального определения.

Приведены примеры на "геометрическую интуицию".
Такого рода задачи под силу пока решить только человеку.

Таким образом, под любой метод кластеризации можно подобрать контрпример
или вырожденный случай, на котором он не будет работать.

\subsection{9. Проблема чувствительности к выборку метрики}

Результат зависит от нормировки признаков:

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

После изменения масштаба по оси $X$ выделись кластеры "высокие студенты"
и "все остальные".

Пунктиром на втором рисунке изображен исходный кластер $B$ из первого
рисунка.

Проблема чувствительности метрики - это не хорошо и не плохо, это свойство
задачи кластеризации.

\subsection{10. Постановка задачи частичного обучения (SSL)}

\textbf{Дано:}
- множество объектов $X$, множество классов $Y$;
- $\begin{matrix}X^k = {\lbrace x_1, \ldots, x_k \rbrace} \text{ - размеченные объекты (labeled data);}\\{\lbrace y_1, \ldots, y_k \rbrace} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \,\end{matrix}$
- $U = {\lbrace x_{k+1}, \ldots, x_l \rbrace}$ - неразмеченные объекты
(unlabeled data).

\textbf{Два варианта постановки задачи:}
- \textit{Частичное обучение} (semi-supervised learning): 
    - построить алгоритм классификации $a \! : X \rightarrow Y$.
- \textit{Трансдуктивное обучение} (transductive learning):
    - зная \textbf{все} ${\lbrace x_{k+1}, \ldots, x_l \rbrace}$, получить
    метки ${\lbrace a_{k+1}, \ldots, a_l \rbrace}$.

\textbf{Типичные приложения:}
- классификация и каталогизация текстов, изображений, и т.п.

Задача является промежуточной между классификацией (не хватает объектов) и
кластеризацией (некоторые объекты уже размечены).

Трансдуктивное обучение - это вывод частного из частного (нужно разметить
неразмеченные объекты выборки, а алгоритм (функцию) строить не нужно).

\subsection{11. SSL не сводится к классификации}

\textbf{Пример 1.}
плотность классов, восстановленные:
- по размеченным данным $X^k$
- по полным даннмы $X^l$

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Слева приведена оценка плотности распределения классов байесовским
классификатором (по пяти объектам каждого класса).

Справа - по всем объектам, а размеченные объекты указывают на то, какому
классу принадлежит соответствующая плотность.

\subsection{12. SSL не сводится к классификации}

\textbf{Пример 2.}
Методы классификации не учитывают кластерную структуру неразмеченных данных

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Задача "Два полумесяца".

Метод ближайших соседей и SVM плохо работают на таких ленточных кластерах
при условии, когда частично размечены самые дальние друг от друга объекты
обучающей выборки (см. квадрат и треугольник).

\subsection{13. SSL также не сводится и к кластеризации}

\textbf{Пример 3.}
Методы кластеризации не учитывают приоритетность разметки над кластерной
структурой.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Нетривиальный контрпример.

При частичном обучении разметка (метки учителя) имеет приоритет над
кластерной структурой объектов выборки.

При кластеризации - наоборот.

\subsection{14. Качество кластеризации в метрическом пространстве}

Пусть известны только попарные расстояния между объектами.
- Среднее внутрикластерное расстояние:
$$\displaystyle F_0 = \frac{\displaystyle \sum_{i<j}{{\left[ a_i = a_j \right]} \rho{( x_i, x_j)}}}{\displaystyle \sum_{i<j}{{\left[ a_i = a_j \right]}}} \rightarrow \min.$$
- Среднее межкластерное расстояние:
$$\displaystyle F_1 = \frac{\displaystyle \sum_{i<j}{{\left[ a_i \neq a_j \right]} \rho{( x_i, x_j)}}}{\displaystyle \sum_{i<j}{{\left[ a_i \neq a_j \right]}}} \rightarrow \max.$$
- Отношение пары функционалов: $F_0 / F_1 \rightarrow \min$.

Эти метрики полезны для оценки качества кластеризации и для выбора
лучшей из них.

Для агрегации двух критериев чаще всего рассматривают частное.

\subsection{15. Качество кластеризации в линейном векторном пространстве}

Пусть объекты $x_i$ задаются векторами ${\left( f_1{(x_i)}, \ldots, f_n{(x_i)} \right)}$.
- Сумма средних внутрикластерных расстояний: $$\Phi_0 = \sum_{a \in Y}{\frac{1}{\vert X_a \vert}} \sum_{i \, : \; a_i = a}{\rho{(x_i, \mu_a)}} \rightarrow \min,$$
    - $X_a = {\lbrace x_i \in X^l \; \vert \; a_i = a \rbrace}$ - кластер $a$,
    - $\mu_a$ - центр масс кластера $a$.
- Сумма межкластерных расстояний: $$\Phi_1 = \sum_{a, b \in Y}{\rho{(\mu_a, \mu_b)}} \rightarrow \max.$$
- Отношение пары функционалов: $\Phi_0 / \Phi_1 \rightarrow \min$.

Предполагается, что векторы (признаки) являются числовыми.

Центры масс не работают, когда кластеры ленточные.
Возможное решение: разбить ленту на части, применив какой-то алгоритм.

Межкластерное расстояние - это расстояние между центрами кластеров.

\subsection{16. Метод K-средних (K-means) для кластеризации}

Минимизация суммы квадратов внутрикластерных расстояний:
$$\displaystyle \sum_{i=1}^{l}{\Vert x_i - \mu_{a_i} \Vert}^2 \rightarrow \min_{{\lbrace a_i \rbrace}, \; {\lbrace \mu_a \rbrace}}, \quad {\Vert x_i - \mu_a \Vert}^2 = \sum_{j=1}^{n}{\left( f_j{(x_i)} - \mu_{a_j} \right)}^2$$

\textbf{Алгоритм Ллойда}
> \textbf{вход:} $X^l, \; K = {\vert Y \vert}$; \textbf{выход:} центры кластеров
$\mu_a, \; a \in Y$;\
$\mu_a \coloneqq$ начальное приближение центров, для всех $a \in Y$;\
\textbf{повторять}
> - отнести каждый $x_i$ к ближайшему центру:$$a_i \coloneqq \min_{a \in Y}{\Vert x_i - \mu_a \Vert}, \quad i = 1, \ldots, l;$$
> - вычислить новые положения центров:$$\mu_a \coloneqq \frac{\sum_{i=1}^{l}{{\left[ a_i = a \right]} x_i}}{\sum_{i=1}^{l}{{\left[ a_i = a \right]}}}, \quad a \in Y;$$
>
> \textbf{пока} $a_i$ не перестанут изменяться;

Минимизируется среднее расстояние до центров кластеров (причем расстояние - Эвклидово, объекты задаются числовыми признаками).

$a_i$ - это номер кластера, к которому алгоритм отнес $x_i$.

$\mu_a$ - это центр $a_i$.

Начальное: выбрать пару самых дальних, потом третий - самый дальний от них
и т.д.

\subsection{17. Метод K-средних (K-means) для частичного обучения}

\textbf{Модификация алгоритма Ллойда} (при наличии размеченных объектов ${\lbrace x_1, \ldots, x_k \rbrace}$)

> \textbf{вход:} $X^l, \; K = {\vert Y \vert}$;\
\textbf{выход:} центры кластеров $\mu_{a}, \; a \in Y$;\
$\mu_a \coloneqq$ начальное приближение центров, для всех $a \in Y$;\
\textbf{повторять}
> - отнести каждый $x_i \in U$ к ближайшему центру:$$a_i \coloneqq \min_{a \in Y}{\Vert x_i - \mu_a \Vert}, \quad i = k+1, \ldots, l;$$
> - вычислить новые положения центров:$$\mu_a \coloneqq \frac{\sum_{i=1}^{l}{{\left[ a_i = a \right]} x_i}}{\sum_{i=1}^{l}{{\left[ a_i = a \right]}}}, \quad a \in Y;$$
>
> \textbf{пока} $a_i$ не перестанут изменяться;

Отличие от предыдущего алгоритма: относить к центрам кластеров нужно только
неразмеченные объекты.

Если объект размечен, его не нужно кластеризовать.

Кластеры будут "обрастать" вокруг размеченных объектов.

\subsection{18. Примеры неудачной кластеризации K-means}

Причина - неудачное начальное приближение или существенная негауссовость
кластеров

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Слева и справа вверху: кластера расщепился из-за того, что все объекты
начального приближения попали в один кластер.

Снизу: $K$-means не подходит для решения таких задач.

Решение проблемы: "растопыренная сетка", мультистарт и проверка результата
по критериям.

\subsection{19. Алгоритм кластеризации DBSCAN}

Объект $x \in U$, его $\varepsilon$-окрестность $U_{\varepsilon}{(x)} = {\lbrace u \in U \! : \rho{(x, u)} \leqslant \varepsilon \rbrace}$

Каждый объект может быть одного из трех типов:
- корневой: имеющий плотную окрестность, ${\vert U_{\varepsilon}{(x)}\vert} \geqslant m$
- граничный: не корневой, но в окрестности корневого
- шумовой (выброс): не корневой и не граничный

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.

Алгоритм подходит для кластеров произвольной формы.

Настраиваются два параметра:
- $\varepsilon$ - окрестность точки (радиус окружности);
- $m$ - количество объектов в окрестности.

\subsection{20. Алгоритм кластеризации DBSCAN}

> \textbf{вход:} выборка $X^l = {\lbrace x_1, \ldots, x_l \rbrace}$; параметры $\varepsilon$ и $m$;\
\textbf{выход:} разбиение выборки на кластеры и шумовые выбросы;\
$U \coloneqq X^l$ - непомеченные; $a \coloneqq 0$;\
\textbf{пока} в выборке есть непомеченные точки, $U \neq \varnothing$:
> - взять случайную точку $x \in U$;
> - \textbf{если} ${\vert U_{\varepsilon}{(x)} \vert} < m$ \textbf{то}
>   - пометить $x$ как, возможно, шумовой;
> - \textbf{иначе}
>   - создать новый кластер: $\quad K \coloneqq U_{\varepsilon}{(x)}; \quad a \coloneqq a + 1$;
>   - \textbf{для всех} $x' \in K$, не помеченных или шумовых
>       - \textbf{если} ${\vert U_{\varepsilon}{(x')} \vert} \geqslant m$ \textbf{то} $K \coloneqq K \cup U_{\varepsilon}{(x')}$;
>       - \textbf{иначе} пометить $x'$ как граничный кластер $K$;
>   - $a_i \coloneqq a$ для всех $x_i \in K$;
>   - $U \coloneqq U \backslash K$;

\subsection{21. Преимущества алгоритма DBSCAN}

- быстрая кластеризация больших данных:
    - $O{(l^2)}$ в худшем случае,
    - $O{(l \ln{l})}$ при эффективной реализации $U_{\varepsilon}{(x)}$;
- кластеры произвольной формы (долой центры!);
- деление объектов на корневые, граничные, шумовые.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Замечание: граница между кластерами простраивается фрагментарно, однако
шумовые объекты определяются довольно точно.

\subsection{22. Агломеративная иерархическая кластеризация}

Алгоритм иерархической кластеризации (Ланс, Уильямс, 1967): итеративный
пересчет расстояний $R_{UV}$ между кластерами $U$, $V$.

> $C_1 \coloneqq {\lbrace {\lbrace x_1 \rbrace}, \ldots, {\lbrace x_l \rbrace} \rbrace}$ - все кластеры 1-элементные;\
> $R_{{\lbrace x_i \rbrace}{\lbrace x_j \rbrace}} \coloneqq \rho{(x_i, x_j)}$ - расстояние между ними;\
\textbf{для всех} $t = 2, \ldots, l$ ($t$ - номер итерации):
> - найти в $C_{t-1}$ пару кластеров ${(U, V)}$ с минимальным $R_{UV}$;
> - слить их в один кластер;
> - $W \coloneqq U \cup V$;
> - $C_t \coloneqq C_{t-1} \cup {\lbrace W \rbrace} \backslash {\lbrace U, V \rbrace}$;
> - \textbf{для всех} $S \in C_t$
>   - вычислить $R_{WS}$ по формуле Ланса-Уильямса:$$R_{WS} \coloneqq \alpha_{U} R_{US} + \alpha_{V} R_{VS} + \beta R_{UV} + \gamma{\vert R_{US} - R_{VS} \vert};$$

Процесс объединения всех элементов в один большой кластер.

Основной вопрос: как померить расстояние между объединенным кластером и
кластером 1-элементным?

Ответ: разными способами, от интуитивных до рациональных.

\subsection{23. Агломеративная иерархическая кластеризация}

- Как определить расстояние $R{(W, S)}$
- между кластерами $W = U \cup V \; \text{и} \; S$,
- зная расстояния $R{(U, S)}$, $R{(V, S)}, R{(U, V)}$?

Формула, обобщающая большинство разумных способов определить это расстояние
\[Ланс, Уильямс, 1967\]:
%$$
%\begin{split}
%R{(U \cup V, S)} = \alpha_{U} &\cdot R{(U, S)} +\\
%+ \alpha_{V} &\cdot R{(V, S)} +\\
%+ \beta &\cdot R{(U, V)} +\\
%+ \gamma &\cdot {\vert R{(U, S)} - R{(V, S)} \vert},
%\end{split}
%$$

где $\alpha_{U}, \alpha_{V}, \beta, \gamma$ - числовые параметры.

\subsection{24. Частные случаи формулы Ланса-Уильямса}

1. \textbf{Расстояние ближнего соседа:}
    - $\displaystyle R_{WS}^{б} = \min_{w \in W, s \in S}{\rho{(w, s)}}$;
    - $\alpha_U = \alpha_V = \frac{1}{2}, \quad \beta = 0, \quad \gamma = -\frac{1}{2}$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

2. \textbf{Расстояние дальнего соседа:}
    - $\displaystyle R_{WS}^{д} = \max_{w \in W, s \in S}{\rho{(w, s)}}$;
    - $\alpha_U = \alpha_V = \frac{1}{2}, \quad \beta = 0, \quad \gamma = \frac{1}{2}$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

3. \textbf{Групповое среднее расстояние:}
    - $\displaystyle R_{WS}^{г} = \frac{1}{{\vert W \vert}{\vert S \vert}} \sum_{w \in W}{\sum_{s \in S}{\rho{(w, s)}}}$;
    - $\alpha_U = \frac{\vert U \vert}{\vert W \vert}, \quad \alpha_V = \frac{\vert V \vert}{\vert W \vert}, \quad \beta = \gamma = 0$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Ближайший сосед работает плохо, а дальний - лучше.

\subsection{25. Частные случаи формулы Ланса-Уильямса}

4. \textbf{Расстояние между центрами:}
    - $\displaystyle R_{WS}^{ц} = \rho^2 {\left( \sum_{w \in W}{\frac{w}{\vert W \vert}}, \sum_{s \in S}{\frac{s}{\vert S \vert}} \right)}$;
    - $\alpha_U = \frac{\vert U \vert}{\vert W \vert}, \quad \alpha_V = \frac{\vert V \vert}{\vert W \vert},$
    - $\beta = - \alpha_U \alpha_V, \quad \gamma = 0.$

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

5. \textbf{Расстояние Уорда:}
    - $\displaystyle R_{WS}^{у} = \frac{{\vert S \vert}{\vert W \vert}}{{\vert S \vert} + {\vert W \vert}} \rho^2 {\left( \sum_{w \in W}{\frac{w}{\vert W \vert}}, \sum_{s \in S}{\frac{s}{\vert S \vert}} \right)}$;
    - $\alpha_U = \frac{{\vert S \vert} + {\vert U \vert}}{{\vert S \vert} + {\vert W \vert}}, \quad \alpha_V = \frac{{\vert S \vert} + {\vert V \vert}}{{\vert S \vert} + {\vert W \vert}}, \quad \beta = \frac{-{\vert S \vert}}{{\vert S \vert} + {\vert W \vert}}, \quad \gamma = 0.$

> \textbf{Проблема выбора}\
Какая функция расстояния лучше?

Расстояние между центрами работает хуже остальных, потому что не обладает
рядом свойств.

Выбирать нужно по визуализации.

\subsection{26. Визуализация кластерной структуры}

1. \textbf{Расстояние ближнего соседа:}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Дендрограмма показывает процесс объединения объектов в кластеры.

\subsection{27. Визуализация кластерной структуры}

2. \textbf{Расстояние дальнего соседа:}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{28. Визуализация кластерной структуры}

3. \textbf{Групповое среднее расстояние:}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{29. Визуализация кластерной структуры}

5. \textbf{Расстояние Уорда:}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{30. Дендрограмма - визуализация иерархической кластеризации}

- Кластеры группируются вдоль горизнотальной оси
- По вертикальной оси откладываются расстояния $R_t$
- Расстояния возрастают, линии нигде не пересекаются
- Верхние уровни различимы лучше, чем нижние
- Уровень отсечения определяет число кластеров

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Самопересечений нет, если каждое следующее расстояние больше предыдущего.