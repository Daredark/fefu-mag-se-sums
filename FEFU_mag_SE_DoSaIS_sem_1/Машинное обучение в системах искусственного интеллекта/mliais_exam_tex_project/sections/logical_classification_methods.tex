\section{Логические методы классификации}

\subsection{2. План лекции}

- Понятия закономерности и информативности
    - Логическая закономерность
    - Интерпретируемость
    - Критерии информативности
- Решающие деревья
    - Алгоритм ID3
    - Небрежные решающие деревья - ODT
    - Бинаризация данных

\subsection{3. Идея логических методов}

Смоделировать человеческую логику принятия решений в ситуациях, когда есть
неточности, нечеткости, прецеденты и т.п.

\textit{Ожидания}: правильность и понятность людям (что весьма полезно в
определенных предметных областях, таких как медицина, геология,
социология, техничечкая диагностика и т.д.).

Идея получить не только решающее правило, но и понять, разумно ли оно, а
также выявить какие \textbf{\textit{логические закономерности}} оно сформировало на
основе имеющихся данных.

\subsection{4. Логическая закономерность (определение и свойства)}

$$X^l = {\left( x_i, y_i \right)}^l_{i=1} \subset X \times Y \text{- обучающая выборка}, \; y_i = y{(x_i)}.$$

\textit{Логическая закономерность} (правило, rule) - это предикат
$R \! : X \rightarrow {\lbrace 0, 1\rbrace}$, удовлетворяющий двум требованиям:

1. \textit{интерпретируемость}:
    1. $R$ записывается на ествественном языке;
    2. $R$ зависит от небольшого числа признаков (1-7);
2. \textit{информативность} относительно одного из классов $c \in Y$ :
    - $p(R) = \# {\lbrace x_i \! : R(x_i)=1 \;\; \text{и} \;\; y_i=c \rbrace} \rightarrow \max$;
    - $n(R) = \# {\lbrace x_i \! : R(x_i)=1 \;\; \text{и} \;\; y_i \neq c \rbrace} \rightarrow \min$;
```
c - предсказательная сила (способность)?
p - positive
n - negative
```

Если $R(x) = 1$, то говорят "$R$ выделяет $x$" ($R$ covers $x$).

\textbf{\textit{Решающее правило}} в данном случае - это совокупность логических
закономерностей.

```
непротиворечивая (чистая) закономерность, которую нужно построить для
каждого класса
```

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

```
противоречивая закономерность (бесполезна, не несет никакой информации о классах)
```

\textbf{Генеральная совокупность} $(X \times Y)$ - это совокупность всех
объектов в рассматриваемой нами предметной области.

$\subset$ - подмножество.

\subsection{5. Логическая закономерность (требования и задачи)}

Отдельная закономерность - это еще не классификатор, это
"недоклассификатор".
Каждый из таких "недоклассификаторов" не может решить задачу, но если
набрать из достаточное количество и построить композицию, то она - сможет.

$\mathbf{n}$ \textbf{\textit{должно быть как можно меньше,}} $\mathbf{p}$
\textbf{\textit{- как можно больше.}}

Эти требования могут входить в противоречия, поэтому необходимо найти
разумный компромисс.

\textbf{Решаемые задачи:}
- Определить, что есть закономерность (на основе статистических критериев).
- Какой вид могут иметь правила (чаще это конъюнкции простых условий).
- Научиться их строить (это, как правило, переборные алгоритмы).
- Понять, как объединять правила в композиции (существует много разных
идей - независимо, последовательно и т.д.).

\subsection{6. Пример реализации свойства интерпретируемости}

1. $R(x)$ записывается на естественном языке;
2. $R(x)$ зависит от небольшого числа признаков (1-7);

```
такие правила соответствуют способу мышления врача или кредитного аналитика
```

\textbf{Пример (из области медицины)}

\textbf{Если} \textit{возраст} $> 60$ \textbf{и} \textit{пациент ранее перенёс инфаркт,} \textbf{то}
\textit{операцию не делать, риск отрицательного исхода 60\%.}

\textbf{Пример (из области кредитного скоринга)}

\textbf{Если} \textit{в анкете указан домашний телефон} \textbf{и} \textit{зарплата} $>$ \textit{\$2000} \textbf{и} \textit{сумма кредита} $<$ \textit{\$5000} \textbf{то}
\textit{кредит можно выдать, риск дефолта 5\%.}

```
естественное накладывание пороговых условий для количественных признаков
```

```
отнесение к классу указано с некоторой "уверенностью" (риски отрицательного
исхода и дефолта); вероятность уверенности находится из обучающей выборки
(доля отрицательных исходов).
```

\subsection{7-14. Как понимают закономерность люди? Тесты М.М. Бонгарда [Проблема узнавания, 1967]}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{15. Что нужно делать, чтобы извлекать закономерности из данных?}

1. Как изобретать признаки $f_1(x), \ldots, f_n(x)$?
    - не наука, а искусство (размышление, озарения, эксперименты,
    консультации, мозговые штурмы, ...)
```
в учебных задачах признаки даны
```
2. Какого вида закономерности $R(x)$ нам нужны?
    - простые формулы от малого числа признаков
```
пространство поиска зависит от задачи
```
3. Как определять информативность?
    - так, чтобы одновременно $p \rightarrow \max$, $n \rightarrow \min$
```
определяем, что является критерием поиска (свертка двух критериев)
```
4. Как искать закономерности?
    - перебором подмножеств признаков
```
эвристики для сокращения полного перебора, который лучше, но долгий
```
5. Как объединять закономерности в алгоритм?
    - любым классификатором ($R(x)$ - это тоже признаки)
```
признак - это тоже функция от объекта, поэтому закономерность может являться
признаком
```

\textit{Закономерность} - интерпретируемый высокоинформативный одноклассовый
классификатор с отказами.

\subsection{16-17. В каком виде ищут закономерности? (часто используемые виды)}

1. \textit{Конъюнкция} пороговых условий (термов):

```
число признаков j должно быть маленьким, чтобы закономерность ...
```

$$R{(x)} = \bigwedge_{j \in J}{\left[ a_j \leqslant f_j{(x)} \leqslant b_j \right].} $$

```
пороговое условие может быть также односторонним
```

2. \textit{Синдром} - когда выполнено не менее $d$ термов из $J$, (при $d = |J|$
это конъюнкция, при $d = 1$ - дизъюнкция):

```
задачи дифференциальной диагностики (должны подтвердиться хотя бы несколько
симптомов)
```

$$R{(x)} = {\left[ \sum_{j \in J}{\left[ a_j \leqslant f_j{(x)} \leqslant b_j \right]} \geqslant d \right]},$$

```
симптом - это признак, синдром - их совокупность для заболевания
```

Синдромы обнаруживаются во многих прикладных областях: в медицинской
диагностике, в кредитном скоринге, в геологическом прогнозировании, и др.

Параметры $J, a_j, b_j, d$ настраиваются по обучающей выборке.

3. \textit{Полуплоскость} - линейная пороговая функция:

```
снова используется небольшое число признаков j (некое подпространство)
```

$$R{(x)} = {\left[ \sum_{j \in J}{w_j f_j{(x)}} \geqslant w_0 \right]}.$$

```
получаем линейную комбинацию признаков (будут рассмотрены далее), а не ∧,
но здесь складываются "км с кг" 
```

4. \textit{Шар} - пороговая функция близости:

```
если вокруг точки x₀ описали шар радиусом w₀, в котором много объектов
одного класса (а других - мало), то это закономерность
```

$$R{(x)} = {\left[ r{(x, x_0)} \leqslant w_0 \right]},$$

```
метрика r, аналог того, что было в метрических методах (эталонность
сравнения)
```

ABO - алгоритмы вычисления оценок \[Ю.И. Журавлёв, 1971\]:

```
способ вычисления оценки
```

$$r{(x, x_0)} = \max_{j \in J}{w_j{\vert f_j{(x)} - f_j{(x_0)} \vert}}.$$

SCM - машины покрывающих множеств \[M. Marchand, 2001\]:

```
способ вычисления оценки
```

$$r{(x, x_0)} = \sum_{j \in J}{w_j{\vert f_j{(x)} - f_j{(x_0)} \vert}^{\gamma}}.$$

```
используется прецедентная логика в проверке и интерпретации результата
```

Параметры $J, w_j, w_0, x_0$ настраиваются по обучающей выборке путем
оптимизации \textit{критерия информативности}.

\subsection{18. Часто используемые критерии информативности}

\textbf{Проблема:} надо сравнивать закономерности
$R$. ```для определения лучшей```

Как свернуть два критерия в один критерий информативности?

$$
\begin{cases}
p{\left( R \right)} \rightarrow \max \\
n{\left( R \right)} \rightarrow \min
\end{cases}
\quad
\xRightarrow{?}
\quad
l{\left( p, n \right)} \rightarrow \max
$$

\textbf{Очевидные, но не всегда адекватные свёртки:}
```
все предложенные здесь меры не очень адекватны, т.к. для них легко найти
контрпримеры (см. след. слайд) ->
```
- $\displaystyle \frac{p}{p+n} \rightarrow \max \quad \text{(precision);}$
```безошибочность```
- $\displaystyle p - n \rightarrow \max \quad \text{(accuracy);}$
```точность```
- $\displaystyle p - Cn \rightarrow \max \quad \text{(linear cost accuracy);}$
```линейная функция стоимости```
- $\displaystyle \frac{p}{P} - \frac{n}{N} \rightarrow \max \quad \text{(relative accuracy);}$ ```относительная точность```

$P = \# {\lbrace x_i \! : y_i = c \rbrace}$ - число "своих" во всей выборке;

$N = \# {\lbrace x_i \! : y_i \neq c \rbrace}$ - число "чужих" во всей выборке.

\subsection{19. Нетривиальность проблемы свертки двух критериев (контрпримеры для них)}

\textbf{Пример.}
```
берем пары предикатов: чтобы первый был явно хорошей закономерностью, а
второй - явно нет, но формула дала бы равную оценку информативности
```

Претенденты на звание "Критерий информативности" при
$P \stackrel{\text{свои}}{=} 200$, $N \stackrel{\text{чужие}}{=} 100$ и
различных $p$ и $n$. ```примеры успешных сверток```

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & Фишер & энтропия & бустинг \\ \hline
        ~ & \$p\$ & \$n\$ & ~ & \$p-n\$ & \$p-5n\$ & \$$\backslash$frac\{p\}\{P\}-$\backslash$frac\{n\}\{N\}\$ & \$$\backslash$frac\{p\}\{n+1\}\$ & ~ & \$	ext\{IStat\}\$ & \$$\backslash$text\{IGain\}\$ & \$$\backslash$sqrt\{p\}-$\backslash$sqrt\{n\}\$ \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        хор. & 50 & 0 & ~ & \_\_50\_\_ & 50 & 0.25 & 50 & ~ & 22.65 & 23.70 & 7.07 \\ \hline
        плох. & 100 & 50 & ~ & \_\_50\_\_ & -150 & 0 & 1.96 & ~ & 2.33 & 1.98 & 2.93 \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        хор. & 50 & 9 & ~ & 41 & \_\_5\_\_ & 0.16 & \_\_5\_\_ & ~ & 7.87 & 7.94 & 4.07 \\ \hline
        плох. & 5 & 0 & ~ & 5 & \_\_5\_\_ & 0.03 & \_\_5\_\_ & ~ & 2.04 & 3.04 & 2.24 \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        хор. & 100 & 0 & ~ & \_\_100\_\_ & 100 & \_\_0.5\_\_ & 100 & ~ & 52.18 & 53.32 & 10.0 \\ \hline
        плох. & 140 & 20 & ~ & \_\_120\_\_ & 40 & \_\_0.5\_\_ & 6.67 & ~ & 37.09 & 37.03 & 7.36 \\ \hline
    \end{tabular}
\end{table}

\textbf{Рассматриваем разные пары правил - хорошее и не очень.}

\textbf{Бустинг} (англ. boosting - улучшение) - это процедура последовательного
построения композиции алгоритмов машинного обучения, когда каждый следующий
алгоритм стремится компенсировать недостатки композиции всех предыдущих
алгоритмов.

\subsection{20. Часто используемые критерии информативности (обзор)}

\textbf{Адекватные, но неочевидные критерии:}
- энтропийный критерий информационного выигрыша:
$$\text{IGain}{\left( p, n \right)} = h{\left( \frac{P}{l} \right)} - \frac{p+n}{l}h{\left( \frac{p}{p+n} \right)} - \frac{l-p-n}{l}h{\left( \frac{P-p}{l-p-n} \right)} \rightarrow \max,$$
где $h(q) = -q \log_2{q} - {\left( 1-q \right)}\log_2{\left( 1-q \right)}$
- точный статистический тест Фишера (Fisher's Exact Test):
$$\text{IStat}{\left( p, n \right)} = -\log_2{C^p_P C^n_N / C^{p+n}_{P+N}} \rightarrow \max$$
- перестановочный статистический тест
- критерий бустинга \[Cohen, Singer, 1999\]:
$$\sqrt{p} - \sqrt{n} \rightarrow \max$$
- нормированный критерий бустинга:
$$\sqrt{p/P} - \sqrt{n/N} \rightarrow \max$$

```
нет однозначного ответа на то, КАКОЙ критерий лучше и КАК сворачивать два
критерия p и n, но есть с десяток разумных способов это сделать
```

\subsection{21. Часто используемые критерии информативности (IGain)}

\textbf{Адекватные, но неочевидные критерии:}

- энтропийный критерий информационного выигрыша:
$$\displaystyle \text{IGain}{\left( p, n \right)} =
\underset{\text{энтропия} \atop \text{всей выборки}}{h{\left( \frac{P}{l} \right)}} -
\stackrel{\text{частота}}{\frac{p+n}{l}}h\underset{\text{энтропия} \atop \text{части выборки}}{\left( \frac{p}{p+n} \right)} -
\stackrel{\text{частота}}{\frac{l-p-n}{l}}h
\underset{\text{энтропия} \atop \text{части выборки}}{\left( \frac{P-p}{l-p-n} \right)} \rightarrow \max,$$
где $h(q) = -q \log_2{q} - {\left( 1-q \right)}\log_2{\left( 1-q \right)}$

\textbf{Информационная энтропия} - мера неопределенности или непредсказуемости
информации, неопределенность появления какого-либо символа алфавита.

Если есть два взаимоисключающие исхода, которым приписаны вероятности
(которые в сумме дают 1), то мы можем связать с этими исходами количество
информации, которое они несут.
"\textit{Чем меньше вероятность исхода, тем больше информации мы получаем, если
этот исход реализуется}".
Энтропия определяется как мат. ожидание количества информации.
Первое событие берем с вероянтностью $\mathbf{q}$, второе $\mathbf{(1-q)}$
и получаем формулу (см. выше).

Вычисляем энтропию, которой обладает обучающая выборка, ДО того, как узнали
предикат $\mathbf{R}$, и ПОСЛЕ того.
Разность этих энтропий покажет, сколько информации $\mathbf{R}$ несет о
делении выборки на классы.
Разность энтропий называется \textit{информационным выигрышем}.

\subsection{22. Часто используемые критерии информативности (IStat)}

\textbf{Адекватные, но неочевидные критерии:}

- точный статистический тест Фишера (Fisher's Exact Test):
$$
\text{IStat}{\left( p, n \right)} =
-\log_2{C^p_P C^n_N / C^{p+n}_{P+N}} \rightarrow \max
$$

```
-log используется для того, чтобы получить величину, которая чем больше, тем
лучше
```

```
Что-то здесь выделяет предикат R
```

$$
\underbrace{
{\overbrace{
\boxed{\begin{matrix}
\varphi{( x_i )} = 1 \\
y_i = c
\end{matrix}}
}^{p}}
\boxed{\begin{matrix}
\varphi{( x_i )} = 0 \\
y_i = c
\end{matrix}}
}_{P}
\underbrace{
{\overbrace{
\boxed{\begin{matrix}
\varphi{( x_i )} = 1 \\
y_i \neq c
\end{matrix}}
}^{n}}
\boxed{\begin{matrix}
\varphi{( x_i )} = 0 \\
y_i \neq c
\end{matrix}}
}_{N}
$$

```
чтобы предикат R был закономерностью, должен быть перекос в сторону p
```

Другая идеология того, как получить оценку информативности - это
статистические тесты.

Пусть предикат $\mathbf{R}$, покрывающий долю объектов выборки, и класс
$\mathbf{c}$, также покрывающий долю объектов, (если трактовать их как
вероятностные события) - это независимые события.

Пусть предикат $\mathbf{R}$ зафиксирован, а у классов есть вероятность
$\mathbf{C^p_{N+P}}$ вариантов распределиться по выборке, и мы считаем все
эти варианты равновероятными.
Т.е. это другой способ сказать, что \textit{предикат} и \textit{класс} - это независимые
случайные величины.

Интуиция говорит, что чтобы $\mathbf{R}$ был закономерностью,
$\mathbf{n/p}$ должно быть много меньше $\mathbf{N/P}$, а статистика выдает
точную количественную формулу (см. выше), которая позволяет судить о том,
насколько не случайно это событие (соответствующее соотношение $\mathbf{n}$
и $\mathbf{p}$).

\subsection{23. Иллюстрация к тому, где находятся закономерности}

Логические закономерности:
$\frac{n}{p+n} \leqslant 0.1, \;\; \frac{p}{P+N} \geqslant 0.05$.
```p-n пространство```

Статистические закономерности:
$\text{IStat}{\left(p, n\right)} \geqslant 3$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{Вывод:} неслучайность - ещё не значит закономерность.

\subsection{24. Парето-критерий информативности в (p, n)-плоскости}

\textbf{Парето-фронт} - множество недоминируемых закономерностей (точка $R$
недоминируема, если правее и ниже точек нет)

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

задача UCI:german

\subsection{24. Как происходит поиск информативных закономерностей?}

Частные случаи:
- стохастический локальный поиск,
- генетические алгоритмы,
- метод ветвей и границ

```
алгоритмов множество, здесь представлена общая идея, под которую подходят
следующие частные случаи
```

\textbf{Вход:} выборка $X^l$;

\textbf{Выход:} множество закономерностей $Z$;

```
общая эвристика, которая может быть реализована по-разному
```

1. начальное множество правил $Z$; ```одно или несколько```
2. \textbf{пока} правила не перестают улучшаться
    - $Z' \coloneqq$ множество модификаций правил $R \in Z$;
    - удалить слишком похожие правила из $Z \cup Z'$;
    - оценить информативность всех правил $R \in Z'$;
    ```по любому критерию```
    - $Z \coloneqq$ наиболее информативные правила из $Z \cup Z'$;
3. \textbf{вернуть} $Z$.

```
по идее, количество правил не должно увеличиваться - оставляем некоторое
изначально заданное их количество
```

\subsection{25. Определение бинарного решающего дерева}

\textit{Бинарное решающее дерево} - алгоритм классификации $a(x)$, задающийся
бинарным деревом:
- $\forall{v} \in V_{\text{внутр}} \rightarrow$ предикат $\beta_v : X \rightarrow {\lbrace 0, 1 \rbrace}, \quad \beta \in B$
- $\forall{v} \in V_{\text{лист}} \rightarrow$ имя класса $c_v \in Y$.
 
1. $v \coloneqq v_0$;
2. \textbf{пока} $v \in V_{внутр}$
3. \textbf{если} $\beta_v{(x)} = 1$ \textbf{то}
4. переход вправо: $v \coloneqq R_v$;
5. \textbf{иначе}
6. переход влево: $v \coloneqq L_v$;
7. \textbf{вернуть} $c_v$.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{26. Пример решающего дерева}

Задача Фишера о классификации цветков ириса на 3 класса, в выборке по 50
объектов каждого класса, 4 признака.

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\textbf{На графике}: в осях двух самых информативных признаков (из 4) два класса
разделились без ошибок, на третьем 3 ошибки.

\subsection{27. Решающее дерево, покрывающее набор конъюнкций}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{28. Пример и задание. Классификации уровня урожайности}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

- $r_?(x) = [\text{Азот} \geqslant 5.35] \wedge [\text{Калий} < 17.7] \wedge [\text{Калий} \geqslant 6.22] \wedge [\text{Фосфор} \geqslant 21.2] \wedge [\text{Азот} < 7.0]$

\subsection{29. Жадный алгоритм построения дерева ID3}

```
в начале передается вся выборка U
```

1. \textbf{ПРОЦЕДУРА} LearnID3 ${\left( U \subseteq X^l \right)}$;
```рекурсивная процедура, берущая на входе часть выборки и строящая поддерево```
2. \textbf{если} все объекты из $U$ лежат в одном классе $c \in Y$ \textbf{то}
3. \textbf{вернуть} новый лист $v, \; c_v \coloneqq c$;
    ```когда нечего расщеплять, создается лист```
4. найти предикат с максимальной информативностью:
$$\beta \coloneqq {\underset{x}{\arg\max}}_{\beta \in B}{I{\left( \beta, U \right)}};$$
```
когда есть что расщеплять, строится функционал β (по критерию ветвления
I), который пытается разбить объекты так, чтобы какие-то классы желательно
целиком ушли в одно из поддеревьев (см. след. слайд)
```
5. разбить выборку на две части $U = U_0 \cup U_1$ по предикату $\beta$:
$$
\begin{matrix}
U_0 \coloneqq {\lbrace x \in U: \; \beta{(x)} = 0 \rbrace};\\
U_1 \coloneqq {\lbrace x \in U: \; \beta{(x)} = 1 \rbrace};
\end{matrix}
$$
6. \textbf{если} $U_0 = \varnothing$ или $U_1 = \varnothing$ \textbf{то}
7. \textbf{вернуть} новый лист
$v, \; c_v \coloneqq \text{Мажоритарный класс}{\left( U \right)}$;
```
построен неинформативный предикат (даже когда получена просто малая
мощность множества);
в этом случае листу приписывается (мажоритарный) класс - которого было
больше в выборке U
```
8. создать новую внутреннюю вершину $v: \; \beta_v \coloneqq \beta$;
```было несколько классов```
$$
\begin{matrix}
\text{построить левое поддерево: } L_v \coloneqq \text{LearnID3}{\left( U_0 \right)};\\
\text{построить правое поддерево: } R_v \coloneqq \text{LearnID3}{\left( U_1 \right)};
\end{matrix}
$$
9. \textbf{вернуть} $v$;

\subsection{30. Разновидности многоклассовых критериев ветвления}

1. \textbf{Отделение одного класса} (слишком сильное ограничение):
$$I{\left( \beta, X^l \right)} = \max_{c \in Y}{I_c}{\left( \beta, X^l \right)}$$
2. \textbf{Многоклассовый энтропийный критерий:}
```
насколько много информации о разделении выборки на классы несет β;
разность энтропии до того, как его узнали, и после дает выигрыш в информации
```
$$
I{\left( \beta, X^l \right)} =
\sum_{c \in Y}{h{\left( \frac{P_c}{l} \right)}} -
\frac{p}{l} \sum_{c \in Y}{h{\left( \frac{p_c}{p} \right)}} -
\frac{l-p}{l} \sum_{c \in Y}{h{\left( \frac{P_c - p_c}{l - p} \right)}},
$$
где
- $P_c = \#{\lbrace x_i \! : \: y_i = c \rbrace}$,
- $p = \#{\lbrace x_i \! : \: \beta{(x_i)} = 1 \rbrace}$,
- $h{(z)} \equiv -z \log_2{z}$.
3. \textbf{Критерий Джинн:}
```
предикат β тем более информативен, чем больше пар объектов, принадлежащих
одному классу, пошли в одно поддерево
```
$$I{\left( \beta, X^l \right)} = \#{\lbrace {\left( x_i, x_j \right)} \! : \: \beta{\left( x_i \right)} = \beta{\left( x_j \right)} \quad \text{и} \quad y_i=y_j \rbrace}.$$
4. $D\textbf{-критерий}$ \textbf{В.И.Донского:}
```
двойственный критерий предикат β тем более информативен, чем больше пар
объектов, принадлежащих разным классам, пошли в разные поддеревья
(разделимость ЛУЧШЕ объединения)
```
$$I{\left( \beta, X^l \right)} = \#{\lbrace {\left( x_i, x_j \right)} \! : \: \beta{\left( x_i \right)} \neq \beta{\left( x_j \right)} \quad \text{и} \quad y_i \neq y_j \rbrace}.$$

\subsection{31. Обработка пропусков (логические алгоритмы толерантны к пропускам)}

\textbf{На стадии обучения:}
```
дерево обладает тем свойством, что можно не знать значения всех признаков,
но тем не менее успешно классифицировать
```
- $\beta_v{(x)}$ не определено $\Rightarrow$ $x_i$ исключается из $U$ для
$I{\left( \beta, U \right)}$
```
если для объекта отсутствует признак, объект исключается из оценки
информативности
```
- $\displaystyle q_v = \frac{|U_0|}{|U|}$ - оценка вероятности левой ветви,
$\forall v \in V_{\text{внутр}}$
```
оценивается вероятность того, что объекты идут по одной или по другой ветке
(для всех ветвей)
```

\textbf{На стадии классификации:}
- $\beta_v{(x)}$ определено $\Rightarrow$ либо налево, либо направо:
$$\displaystyle P_v{(y|x)} = {\left( 1 - \beta_v{(x)} \right)} P_{L_v}{\left( y|x \right)} + \beta_v{\left( x \right)} P_{R_v}{\left( y|x \right)}.$$
\textit{четко}: предикат $\beta_v{(x)}=1$, значит он выделил объект $x$.

- $\beta_v{(x)}$ не определено $\Rightarrow$
\textit{пропорциональное распределение}:
$$\displaystyle P_v{(y|x)} = q_v P_{L_v}{\left( y|x \right)} + {\left( 1-q_v \right)} P_{R_v}{\left( y|x \right)}.$$

\textit{размыто}: считаем вероятность дочерней вершины.

- Окончательное решение - наиболее вероятный класс:
$$y = {\underset{x}{\arg\max}}_{y \in Y}{P_{v_0}}{\left( y|x \right)}.$$

```
в терминальной вершине может быть как метка одного класса, так и
распределение нескольких (в случае, если при обучениии в вершину
пришла подвыборка с объектами разных классов)
```

\subsection{32. Решающие деревья ID3: достоинства и недостатки}

\textbf{Достоинства:}
- Интерпретируемость и простота классификации.
- Гибкость: можно варьировать множество $B$.
- Допустимы разнотипные данные и данные с пропусками.
- Трудоемкость линейна по длине выборки $O{\left( {|B|} hl \right)}$.
- Не бывает отказов от классификации. ```наилучшее дерево можно найти полный перебором```

```
условия могут быть любого типа, при этом любой признак любого типа можно
бинаризовать
```
\textbf{Недостатки:}
- Жадный ID3 переусложняет струткуру дерева, и, как следствие, сильно
переобучается. ```иногда листы построены по малым (статистически ненадежным) подвыборкам```
- Фрагментация выборки: чем дальше $v$ от корня, тем меньше статистическая
надежность выбора $\beta_v$, $c_v$.
- Высокая чувствительность к шуму, к составу выборки, к критерию
информативности.

\subsection{33. Иллюстрация того, что каждый ID3 переусложняет структуру дерева}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{34. Редукция дерева ("стрижка кустов", pruning: C4.5, CART)}

```
хотим понять, какие вершины оказались лишними - до которых не дошел ни один
объект контрольной выборки
```

$X^k$ - независимая контрольная выборка, $k \approx 0.5l$.
1. \textbf{для всех} $v \in V_{\text{внутр}}$
2. $S_v \coloneqq$ подмножество объектов $X^k$, дошедших до $v$;
3. \textbf{если} $S_v = \varnothing$ \textbf{то} ```от этой вершины удаляется поддерево и она заменяется на лист```
4. \textbf{вернуть} новый лист $v, \; c_v \coloneqq \text{Мажоритарный класс}{\left( U \right)}$;
5. число ошибок при классификации $S_v$ четырьмя способами:
    - $r{(v)}$ - поддеревом, растущим из вершины $v$;
    - $r_L{(v)}$ - поддеревом левой дочерней вершины $L_v$;
    - $r_R{(v)}$ - поддеревом правой дочерней вершины $R_v$;
    - $r_c{(v)}$ - к классу $c \in Y$.
```
если до этой вершины дошла какая-то часть объектов, считаем число ошибок
```
6. в зависимости от того, какое из них минимально:
    - сохранить поддерево $v$;
    - заменить поддерево $v$ поддеревом $L_v$; ```срезали правое поддерево```
    - заменить поддерево $v$ поддеревом $R_v$; ```срезали левое поддерево```
    - заменить поддерево $v$ листом, $\displaystyle c_v \coloneqq {\underset{x}{\arg\min}}_{c \in Y}{r_c{\left( v \right)}}$. ```срезали все```
```
вопросы при реализации: в каком порядке обходить, сверху или снизу, случайно
или по критериям и т.п.
```

\subsection{35. Небрежные решающие деревья - ODT (Oblivious Decision Tree)}

```
еще одна конструкция с идеей строить деревья быстро
```

\textbf{Решение проблемы фрагментации}:

- строится сбалансированное дерево высоты $H$;
- для всех узлов уровня $h$ условие ветвления $\beta_h{(x)}$ \textit{одинаково};
- на уровне $h$ ровно $2^{h-1}$ вершин;
- $X$ делится на $2^H$ ячеек.

Классификатор задается \textit{таблицей решений}
$T : {\lbrace 0, 1 \rbrace}^H \rightarrow Y$:
$$a{(x)} = T{\left( \beta_1{(x)}, \ldots, \beta_H{(x)} \right)}.$$

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

\subsection{Разная информация}

\includegraphics[scale=0.3]{figures/samplefigure.jpg}

Ответы на некоторые вопросы:
- Название файлов для сдачи: ```фамилия\_групп\_ММО\_№.ipynb```
- Задания сдавать в отдельных файлах (метрические методы - в первом файле,
логические методы - во втором и т.д.)
- Зачем нам процент ошибок на обучении?
Мы смотрим процент ошибок на обучении, чтобы узнать насколько разделимы
классы.
\textbf{Ожидать} процент ошибок на контроле меньше, чем на обучении, не стоит.
- В линейных методах: learning rate и velocity.

Чаще всего в обзоре литературы то - что было сделано до нас.
Ещё нужно сформировать некоторые ожидаемые критерии, которыми
будет оцениваться наша программа.

Посмотреть описания алгоритмов KNN и Parzen Window (только для меня).